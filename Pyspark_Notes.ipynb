{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efe5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "#Importing the Libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d82f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark Session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"PySparkPractice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv files with PySpark\n",
    "df = spark.read.csv('datacamp_ecommerce.csv',header=True,escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Imported Dataset\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a New Column\n",
    "df = df.withColumn('Weight in Kg', df.Weight/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the Column\n",
    "df = df.withColumnRenamed(\"Weight in Kg\", \"Weight in Kilograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8da654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the column\n",
    "df.select(df.Weight, df['Weight in Kilograms']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Column Column Alias\n",
    "df.select(df['Weight in Kilograms'].alias(\"Kilograms\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different ways of type conversion of pyspark\n",
    "df2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n",
    "    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n",
    "    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = df2.selectExpr(\"cast(age as int) age\",\n",
    "    \"cast(isGraduated as string) isGraduated\",\n",
    "    \"cast(jobStartDate as string) jobStartDate\")\n",
    "df3.printSchema()\n",
    "df3.show(truncate=False)\n",
    "\n",
    "df3.createOrReplaceTempView(\"CastExample\")\n",
    "df4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\n",
    "df4.printSchema()\n",
    "df4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing arithmetic operations\n",
    "\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping and sorting data\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "df.groupBy(\"state\") \\\n",
    "  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n",
    "  .filter(col(\"sum_salary\") > 100000)  \\\n",
    "  .sort(desc(\"sum_salary\")) \\\n",
    "  .show()\n",
    "# Sory by on group by column\n",
    "from pyspark.sql.functions import asc\n",
    "dfFilter.sort(\"sum_salary\").show()\n",
    "# Sort by descending order.\n",
    "from pyspark.sql.functions import desc\n",
    "dfFilter.sort(desc(\"sum_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate Functions\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "#collect_list - with duplicates\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "\n",
    "#collect_set - without duplicates\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "#countDistinct\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "\n",
    "#count\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "\n",
    "#first\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "\n",
    "#last\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a50cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-Processing with PySpark\n",
    "#drop null values\n",
    "df.na.drop(\"any\").show()\n",
    "\n",
    "# Using dropDuplicates on multiple columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "\n",
    "\n",
    "# Using dropDuplicates on single column\n",
    "dropDisDF = df.dropDuplicates([\"salary\"]).select(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with datetime values\n",
    "#current_date()\n",
    "df.select(current_date().alias(\"current_date\")\n",
    "  ).show(1)\n",
    "#date_format()\n",
    "df.select(col(\"input\"), \n",
    "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n",
    "  ).show()\n",
    "#to_date()\n",
    "df.select(col(\"input\"), \n",
    "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n",
    "  ).show()\n",
    "#datediff()\n",
    "df.select(col(\"input\"), \n",
    "    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n",
    "  ).show()\n",
    "#months_between()\n",
    "df.select(col(\"input\"), \n",
    "    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n",
    "  ).show()\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d969c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window functions : ranking, aggregate functions\n",
    "#aggregate function\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Two DataFrames\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "  empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\")\n",
    "    .show(truncate=False)\n",
    "  empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\")\n",
    "    .show(truncate=False)\n",
    "    \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Join on multiple DataFrames\n",
    "df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
    "   .join(df3,df1.id1 == df3.id3,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA) using Pyspark\n",
    "\n",
    "#dataframe schema\n",
    "sales_df.printSchema()\n",
    "\n",
    "#display list of columns\n",
    "sales_df.columns\n",
    "\n",
    "#filter condition with selective columns\n",
    "country_df.select(‘COUNTRY_ID’,\n",
    " ‘COUNTRY_ISO_CODE’,\n",
    " ‘COUNTRY_NAME’,).filter(country_df.COUNTRY_NAME==’India’).show()\n",
    "\n",
    "#GroupBy and Aggregation\n",
    "cust_wise_df=sale_sum_df.groupBy(round(‘CUST_ID’,0).alias(‘CUST_ID’), \n",
    "                                 year(sale_sum_df[‘TIME_ID’]).alias(‘YEAR’)).sum(‘AMOUNT_SOLD’)\n",
    "\n",
    "#Data Sorting\n",
    "cust_wise_df.orderBy(cust_wise_df.CUST_ID).show(15)\n",
    "cust_wise_df.filter(cust_wise_df.CUST_ID==3261).show()\n",
    "\n",
    "#Data Insights\n",
    "#find out which channel contributed most to the sales\n",
    "c_df=chan_df.select(col(‘CHANNEL_ID’).alias(‘CHANNEL_ID_C’),col(‘CHANNEL_DESC’).alias(‘CHANNEL_NAME’))\n",
    "sa_df=sales_df.select(col(‘CHANNEL_ID’).alias(‘CHANNEL_ID_S’),’AMOUNT_SOLD’)\n",
    "chan_sales_df=sa_df.join(c_df,c_df.CHANNEL_ID_C==sa_df.CHANNEL_ID_S,how=’inner’)\n",
    "chan_sale=chan_sales_df.groupBy(round(‘CHANNEL_ID_C’,0).alias(‘CHANNEL_ID’)).sum(‘AMOUNT_SOLD’)\n",
    "chan_top_sales=chan_sale.withColumnRenamed(‘sum(AMOUNT_SOLD)’,’TOT_AMOUNT’)\n",
    "chan_top_sales.orderBy(‘CHANNEL_ID’).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f88f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7cfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81ce3e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "list=[1,2,3,4,5]\n",
    "print(list[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d38b4563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'one', 2: 'two', 3: 'three'}\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "dict1={1:\"one\",2:\"two\",3:\"three\"}\n",
    "print(dict1)\n",
    "val=2\n",
    "if val in dict1.keys():\n",
    "    print(dict1[val])\n",
    "else:\n",
    "    print(\"does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aeb0f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "x=lambda i:i*i\n",
    "print(x(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082977b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
